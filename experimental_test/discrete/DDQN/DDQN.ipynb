{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Double DQNの解説\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "画像引用:  \n",
    "https://qiita.com/sugulu/items/3c7d6cbe600d455e853b\n",
    "\n",
    "### DQNの特徴\n",
    "- Q学習において状態行動テーブルを関数で表したもの.\n",
    "- 離散的な行動を扱うことができる.\n",
    "\n",
    "参考:  \n",
    "http://blog.syundo.org/post/20171208-reinforcement-learning-dqn-and-impl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### OpenAI gymのインストール\n",
    "\n",
    "githubのレポジトリを参考に, gymモジュールをインストールしてください.  \n",
    "https://github.com/openai/gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import renom as rm\n",
    "import matplotlib.pyplot as plt\n",
    "from renom.utility.initializer import Gaussian\n",
    "from renom.cuda import set_cuda_active\n",
    "from renom_rl.discrete.double_dqn import DoubleDQN\n",
    "from renom_rl.environ import BaseEnv\n",
    "from gym.core import Env\n",
    "from PIL import Image\n",
    "\n",
    "set_cuda_active(True)\n",
    "env = gym.make('BreakoutNoFrameskip-v4')\n",
    "\n",
    "class CustomEnv(BaseEnv):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_shape = 4\n",
    "        self.state_shape = (4, 84, 84)\n",
    "        self.previous_frames = []\n",
    "        self._reset_flag = True\n",
    "        self._last_live = 5\n",
    "        super(CustomEnv, self).__init__()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self._reset_flag:\n",
    "            self._reset_flag = False\n",
    "            self.env.reset()\n",
    "        n_step = np.random.randint(4, 32+1)\n",
    "        for _ in range(n_step):\n",
    "            state, _, _ = self.step(self.env.action_space.sample())\n",
    "        return state\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def _preprocess(self, state):\n",
    "        resized_image = Image.fromarray(state).resize((84, 110)).convert('L')\n",
    "        image_array = np.asarray(resized_image)/255.\n",
    "        final_image = image_array[26:110]\n",
    "        # Confirm that the image is processed correctly.\n",
    "        # Image.fromarray(np.clip(final_image.reshape(84, 84)*255, 0, 255).astype(np.uint8)).save(\"test.png\")\n",
    "        return final_image\n",
    "    \n",
    "    def step(self, action):\n",
    "        state_list = []\n",
    "        reward_list = []\n",
    "        terminal = False\n",
    "        for _ in range(4):\n",
    "            # Use last frame. Other frames will be skipped.\n",
    "            s, r, t, info = self.env.step(action)\n",
    "            state = self._preprocess(s)\n",
    "            reward_list.append(r)\n",
    "            if self._last_live > info[\"ale.lives\"]:\n",
    "                t = True\n",
    "                self._last_live = info[\"ale.lives\"]\n",
    "                if self._last_live > 0:\n",
    "                    self._reset_flag = False\n",
    "                else:\n",
    "                    self._last_live = 5\n",
    "                    self._reset_flag = True\n",
    "            if t:\n",
    "                terminal = True\n",
    "                \n",
    "        if len(self.previous_frames) > 3:\n",
    "            self.previous_frames = self.previous_frames[1:] + [state]\n",
    "        else:\n",
    "            self.previous_frames += [state]\n",
    "        state = np.stack(self.previous_frames)\n",
    "        return state, np.array(np.sum(reward_list) > 0), terminal\n",
    "    \n",
    "custom_env = CustomEnv(env)\n",
    "q_network = rm.Sequential([rm.Conv2d(32, filter=8, stride=4, ignore_bias=False),\n",
    "                           rm.Relu(),\n",
    "                           rm.Conv2d(64, filter=4, stride=2, ignore_bias=False),\n",
    "                           rm.Relu(),\n",
    "                           rm.Conv2d(64, filter=3, stride=1, ignore_bias=False),\n",
    "                           rm.Relu(), \n",
    "                           rm.Flatten(), \n",
    "                           rm.Dense(512, ignore_bias=True),\n",
    "                           rm.Relu(),\n",
    "                           rm.Dense(custom_env.action_shape, ignore_bias=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = DoubleDQN(custom_env, q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run random 50000 step for storing experiences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 001 avg_loss:0.0031 total reward in epoch: [train:109.000 test: 3.0] avg reward in episode:[train:0.235 test:0.039] e-greedy:0.900: 100%|██████████| 10000/10000 [05:48<00:00, 28.73it/s]\n",
      "epoch 002 avg_loss:0.0006 total reward in epoch: [train:116.000 test:14.0] avg reward in episode:[train:0.284 test:0.163] e-greedy:0.900: 100%|██████████| 10000/10000 [05:53<00:00, 25.70it/s]\n",
      "epoch 003 avg_loss:0.0007 total reward in epoch: [train:112.000 test:10.0] avg reward in episode:[train:0.250 test:0.145] e-greedy:0.900: 100%|██████████| 10000/10000 [05:54<00:00, 28.24it/s]\n",
      "epoch 004 avg_loss:0.0006 total reward in epoch: [train:90.000 test: 7.0] avg reward in episode:[train:0.188 test:0.125] e-greedy:0.900: 100%|██████████| 10000/10000 [05:54<00:00, 30.60it/s]\n",
      "epoch 005 avg_loss:0.0006 total reward in epoch: [train:119.000 test:20.0] avg reward in episode:[train:0.306 test:0.227] e-greedy:0.900: 100%|██████████| 10000/10000 [05:57<00:00, 27.95it/s]\n",
      "epoch 006 avg_loss:0.0006 total reward in epoch: [train:121.000 test: 4.0] avg reward in episode:[train:0.297 test:0.103] e-greedy:0.900: 100%|██████████| 10000/10000 [05:53<00:00, 27.13it/s]\n",
      "epoch 007 avg_loss:0.0005 total reward in epoch: [train:96.000 test: 4.0] avg reward in episode:[train:0.227 test:0.047] e-greedy:0.900: 100%|██████████| 10000/10000 [05:05<00:00, 29.47it/s]\n",
      "epoch 008 avg_loss:0.0005 total reward in epoch: [train:108.000 test:18.0] avg reward in episode:[train:0.252 test:0.217] e-greedy:0.900: 100%|██████████| 10000/10000 [05:03<00:00, 34.35it/s]\n",
      "epoch 009 avg_loss:0.0005 total reward in epoch: [train:86.000 test:29.0] avg reward in episode:[train:0.209 test:0.408] e-greedy:0.900: 100%|██████████| 10000/10000 [04:50<00:00, 34.43it/s]\n",
      "epoch 010 avg_loss:0.0005 total reward in epoch: [train:102.000 test:10.0] avg reward in episode:[train:0.244 test:0.112] e-greedy:0.900: 100%|██████████| 10000/10000 [04:51<00:00, 34.29it/s]\n",
      "epoch 011 avg_loss:0.0006 total reward in epoch: [train:89.000 test:25.0] avg reward in episode:[train:0.217 test:0.595] e-greedy:0.900: 100%|██████████| 10000/10000 [04:57<00:00, 33.57it/s]\n",
      "epoch 012 avg_loss:0.0006 total reward in epoch: [train:95.000 test:29.0] avg reward in episode:[train:0.229 test:0.397] e-greedy:0.900: 100%|██████████| 10000/10000 [04:59<00:00, 33.39it/s]\n",
      "epoch 013 avg_loss:0.0008 total reward in epoch: [train:98.000 test: 7.0] avg reward in episode:[train:0.244 test:0.115] e-greedy:0.900: 100%|██████████| 10000/10000 [04:53<00:00, 34.05it/s]\n",
      "epoch 014 avg_loss:0.0009 total reward in epoch: [train:112.000 test:10.0] avg reward in episode:[train:0.262 test:0.164] e-greedy:0.900: 100%|██████████| 10000/10000 [04:55<00:00, 33.85it/s]\n",
      "epoch 015 avg_loss:0.0009 total reward in epoch: [train:107.000 test:28.0] avg reward in episode:[train:0.281 test:0.338] e-greedy:0.900: 100%|██████████| 10000/10000 [05:00<00:00, 33.33it/s]\n",
      "epoch 016 avg_loss:0.0010 total reward in epoch: [train:112.000 test:36.0] avg reward in episode:[train:0.265 test:0.921] e-greedy:0.900: 100%|██████████| 10000/10000 [04:48<00:00, 41.19it/s]\n",
      "epoch 017 avg_loss:0.0012 total reward in epoch: [train:119.000 test: 6.0] avg reward in episode:[train:0.322 test:0.102] e-greedy:0.900: 100%|██████████| 10000/10000 [04:57<00:00, 33.66it/s]\n",
      "epoch 018 avg_loss:0.0014 total reward in epoch: [train:131.000 test:11.0] avg reward in episode:[train:0.390 test:0.143] e-greedy:0.900: 100%|██████████| 10000/10000 [04:40<00:00, 35.71it/s]\n",
      "epoch 019 avg_loss:0.0013 total reward in epoch: [train:165.000 test:33.0] avg reward in episode:[train:0.632 test:0.717] e-greedy:0.900: 100%|██████████| 10000/10000 [04:59<00:00, 33.43it/s]\n",
      "epoch 020 avg_loss:0.0015 total reward in epoch: [train:161.000 test:41.0] avg reward in episode:[train:0.590 test:0.826] e-greedy:0.900: 100%|██████████| 10000/10000 [05:06<00:00, 29.25it/s]\n",
      "epoch 021 avg_loss:0.0016 total reward in epoch: [train:164.000 test:26.0] avg reward in episode:[train:0.620 test:0.591] e-greedy:0.900: 100%|██████████| 10000/10000 [05:22<00:00, 30.99it/s]\n",
      "epoch 022 avg_loss:0.0018 total reward in epoch: [train:188.000 test:37.0] avg reward in episode:[train:0.714 test:0.740] e-greedy:0.900: 100%|██████████| 10000/10000 [04:50<00:00, 34.37it/s]\n",
      "epoch 023 avg_loss:0.0020 total reward in epoch: [train:186.000 test:38.0] avg reward in episode:[train:0.694 test:0.698] e-greedy:0.900: 100%|██████████| 10000/10000 [04:47<00:00, 34.81it/s]\n",
      "epoch 024 avg_loss:0.0022 total reward in epoch: [train:186.000 test:37.0] avg reward in episode:[train:0.729 test:0.841] e-greedy:0.900: 100%|██████████| 10000/10000 [04:57<00:00, 30.48it/s]\n",
      "epoch 025 avg_loss:0.0025 total reward in epoch: [train:189.000 test:41.0] avg reward in episode:[train:0.891 test:0.727] e-greedy:0.900: 100%|██████████| 10000/10000 [04:59<00:00, 33.36it/s]\n",
      "epoch 026 avg_loss:0.0027 total reward in epoch: [train:219.000 test:42.0] avg reward in episode:[train:1.191 test:1.176] e-greedy:0.900: 100%|██████████| 10000/10000 [04:20<00:00, 38.36it/s]\n",
      "epoch 027 avg_loss:0.0032 total reward in epoch: [train:223.000 test:41.0] avg reward in episode:[train:1.168 test:1.323] e-greedy:0.900: 100%|██████████| 10000/10000 [03:40<00:00, 45.32it/s]\n",
      "epoch 028 avg_loss:0.0033 total reward in epoch: [train:217.000 test:44.0] avg reward in episode:[train:1.428 test:1.079] e-greedy:0.900: 100%|██████████| 10000/10000 [03:56<00:00, 42.21it/s]\n",
      "epoch 029 avg_loss:0.0039 total reward in epoch: [train:218.000 test:37.0] avg reward in episode:[train:1.517 test:1.423] e-greedy:0.900: 100%|██████████| 10000/10000 [04:16<00:00, 39.03it/s]\n",
      "epoch 030 avg_loss:0.0042 total reward in epoch: [train:239.000 test:47.0] avg reward in episode:[train:1.763 test:1.552] e-greedy:0.900: 100%|██████████| 10000/10000 [04:45<00:00, 35.09it/s]\n",
      "epoch 031 avg_loss:0.0043 total reward in epoch: [train:235.000 test:46.0] avg reward in episode:[train:1.752 test:2.812] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00, 35.24it/s]\n",
      "epoch 032 avg_loss:0.0046 total reward in epoch: [train:241.000 test:47.0] avg reward in episode:[train:1.934 test:1.731] e-greedy:0.900: 100%|██████████| 10000/10000 [04:50<00:00, 34.44it/s]\n",
      "epoch 033 avg_loss:0.0047 total reward in epoch: [train:232.000 test:55.0] avg reward in episode:[train:1.724 test:2.000] e-greedy:0.900: 100%|██████████| 10000/10000 [04:39<00:00, 35.73it/s]\n",
      "epoch 034 avg_loss:0.0048 total reward in epoch: [train:235.000 test:54.0] avg reward in episode:[train:1.836 test:2.409] e-greedy:0.900: 100%|██████████| 10000/10000 [04:39<00:00, 36.47it/s]\n",
      "epoch 035 avg_loss:0.0047 total reward in epoch: [train:238.000 test:44.0] avg reward in episode:[train:2.052 test:1.750] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00, 32.37it/s]\n",
      "epoch 036 avg_loss:0.0046 total reward in epoch: [train:233.000 test:45.0] avg reward in episode:[train:1.713 test:1.375] e-greedy:0.900: 100%|██████████| 10000/10000 [05:03<00:00, 37.55it/s]\n",
      "epoch 037 avg_loss:0.0046 total reward in epoch: [train:244.000 test:51.0] avg reward in episode:[train:2.103 test:2.450] e-greedy:0.900: 100%|██████████| 10000/10000 [04:59<00:00, 33.44it/s]\n",
      "epoch 038 avg_loss:0.0045 total reward in epoch: [train:240.000 test:48.0] avg reward in episode:[train:2.194 test:1.643] e-greedy:0.900: 100%|██████████| 10000/10000 [04:53<00:00, 34.06it/s]\n",
      "epoch 039 avg_loss:0.0043 total reward in epoch: [train:241.000 test:49.0] avg reward in episode:[train:2.360 test:2.095] e-greedy:0.900: 100%|██████████| 10000/10000 [04:57<00:00, 33.64it/s]\n",
      "epoch 040 avg_loss:0.0043 total reward in epoch: [train:239.000 test:51.0] avg reward in episode:[train:2.173 test:2.125] e-greedy:0.900: 100%|██████████| 10000/10000 [04:55<00:00, 33.84it/s]\n",
      "epoch 041 avg_loss:0.0042 total reward in epoch: [train:227.000 test:47.0] avg reward in episode:[train:1.744 test:2.316] e-greedy:0.900: 100%|██████████| 10000/10000 [04:42<00:00, 35.39it/s]\n",
      "epoch 042 avg_loss:0.0041 total reward in epoch: [train:237.000 test:52.0] avg reward in episode:[train:1.975 test:2.824] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00, 35.32it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 043 avg_loss:0.0039 total reward in epoch: [train:240.000 test:49.0] avg reward in episode:[train:1.983 test:2.611] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00,  1.99it/s]\n",
      "epoch 044 avg_loss:0.0041 total reward in epoch: [train:243.000 test:54.0] avg reward in episode:[train:1.875 test:3.188] e-greedy:0.900: 100%|██████████| 10000/10000 [04:57<00:00, 33.62it/s]\n",
      "epoch 045 avg_loss:0.0040 total reward in epoch: [train:236.000 test:52.0] avg reward in episode:[train:1.958 test:3.333] e-greedy:0.900: 100%|██████████| 10000/10000 [04:44<00:00, 34.07it/s]\n",
      "epoch 046 avg_loss:0.0040 total reward in epoch: [train:240.000 test:48.0] avg reward in episode:[train:1.745 test:2.087] e-greedy:0.900: 100%|██████████| 10000/10000 [04:41<00:00, 35.49it/s]\n",
      "epoch 047 avg_loss:0.0040 total reward in epoch: [train:244.000 test:46.0] avg reward in episode:[train:2.440 test:1.917] e-greedy:0.900: 100%|██████████| 10000/10000 [04:42<00:00,  1.31it/s]\n",
      "epoch 048 avg_loss:0.0042 total reward in epoch: [train:236.000 test:51.0] avg reward in episode:[train:2.388 test:2.238] e-greedy:0.900: 100%|██████████| 10000/10000 [04:50<00:00, 34.45it/s]\n",
      "epoch 049 avg_loss:0.0040 total reward in epoch: [train:241.000 test:52.0] avg reward in episode:[train:2.434 test:2.474] e-greedy:0.900: 100%|██████████| 10000/10000 [05:00<00:00, 33.33it/s]\n",
      "epoch 050 avg_loss:0.0041 total reward in epoch: [train:242.000 test:53.0] avg reward in episode:[train:2.104 test:4.900] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00, 35.23it/s]\n",
      "epoch 051 avg_loss:0.0040 total reward in epoch: [train:242.000 test:44.0] avg reward in episode:[train:2.286 test:3.308] e-greedy:0.900: 100%|██████████| 10000/10000 [04:46<00:00, 34.95it/s]\n",
      "epoch 052 avg_loss:0.0039 total reward in epoch: [train:243.000 test:54.0] avg reward in episode:[train:2.142 test:2.789] e-greedy:0.900: 100%|██████████| 10000/10000 [04:59<00:00, 33.43it/s]\n",
      "epoch 053 avg_loss:0.0041 total reward in epoch: [train:245.000 test:40.0] avg reward in episode:[train:2.159 test:1.200] e-greedy:0.900: 100%|██████████| 10000/10000 [05:27<00:00, 37.61it/s]\n",
      "epoch 054 avg_loss:0.0040 total reward in epoch: [train:250.000 test:53.0] avg reward in episode:[train:2.362 test:3.571] e-greedy:0.900: 100%|██████████| 10000/10000 [04:36<00:00, 36.23it/s]\n",
      "epoch 055 avg_loss:0.0040 total reward in epoch: [train:256.000 test:49.0] avg reward in episode:[train:2.419 test:2.938] e-greedy:0.900: 100%|██████████| 10000/10000 [04:41<00:00, 35.57it/s]\n",
      "epoch 056 avg_loss:0.0040 total reward in epoch: [train:246.000 test:50.0] avg reward in episode:[train:2.568 test:2.381] e-greedy:0.900: 100%|██████████| 10000/10000 [04:40<00:00, 35.67it/s]\n",
      "epoch 057 avg_loss:0.0040 total reward in epoch: [train:257.000 test:55.0] avg reward in episode:[train:2.802 test:2.562] e-greedy:0.900: 100%|██████████| 10000/10000 [04:39<00:00, 42.75it/s]\n",
      "epoch 058 avg_loss:0.0041 total reward in epoch: [train:256.000 test:54.0] avg reward in episode:[train:2.451 test:2.650] e-greedy:0.900: 100%|██████████| 10000/10000 [04:44<00:00, 35.17it/s]\n",
      "epoch 059 avg_loss:0.0043 total reward in epoch: [train:263.000 test:50.0] avg reward in episode:[train:3.095 test:2.350] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00, 35.29it/s]\n",
      "epoch 060 avg_loss:0.0043 total reward in epoch: [train:268.000 test:55.0] avg reward in episode:[train:2.891 test:3.467] e-greedy:0.900: 100%|██████████| 10000/10000 [04:45<00:00, 42.44it/s]\n",
      "epoch 061 avg_loss:0.0045 total reward in epoch: [train:261.000 test:53.0] avg reward in episode:[train:3.095 test:2.650] e-greedy:0.900: 100%|██████████| 10000/10000 [04:49<00:00, 34.49it/s]\n",
      "epoch 062 avg_loss:0.0045 total reward in epoch: [train:273.000 test:54.0] avg reward in episode:[train:2.687 test:3.400] e-greedy:0.900: 100%|██████████| 10000/10000 [04:58<00:00, 33.48it/s]\n",
      "epoch 063 avg_loss:0.0049 total reward in epoch: [train:266.000 test:53.0] avg reward in episode:[train:2.944 test:2.529] e-greedy:0.900: 100%|██████████| 10000/10000 [04:41<00:00, 35.52it/s]\n",
      "epoch 064 avg_loss:0.0052 total reward in epoch: [train:266.000 test:50.0] avg reward in episode:[train:3.081 test:3.571] e-greedy:0.900: 100%|██████████| 10000/10000 [04:43<00:00, 35.29it/s]\n",
      "epoch 065 avg_loss:0.0052 total reward in epoch: [train:271.000 test:51.0] avg reward in episode:[train:3.506 test:3.846] e-greedy:0.900: 100%|██████████| 10000/10000 [04:47<00:00, 34.83it/s]\n",
      "epoch 066 avg_loss:0.0055 total reward in epoch: [train:264.000 test:57.0] avg reward in episode:[train:2.966 test:4.455] e-greedy:0.900: 100%|██████████| 10000/10000 [05:04<00:00, 32.84it/s]\n",
      "epoch 067 avg_loss:0.0057 total reward in epoch: [train:272.000 test:48.0] avg reward in episode:[train:2.883 test:2.368] e-greedy:0.900: 100%|██████████| 10000/10000 [04:37<00:00, 35.34it/s]\n",
      "epoch 068 avg_loss:0.0058 total reward in epoch: [train:269.000 test:57.0] avg reward in episode:[train:3.205 test:5.500] e-greedy:0.900: 100%|██████████| 10000/10000 [04:47<00:00, 34.80it/s]\n",
      "epoch 069 avg_loss:0.0064 total reward in epoch: [train:271.000 test:54.0] avg reward in episode:[train:3.229 test:3.000] e-greedy:0.900: 100%|██████████| 10000/10000 [04:42<00:00, 35.38it/s]\n",
      "epoch 070 avg_loss:0.0067 total reward in epoch: [train:267.000 test:47.0] avg reward in episode:[train:2.673 test:3.750] e-greedy:0.900: 100%|██████████| 10000/10000 [04:49<00:00, 34.59it/s]\n",
      "epoch 071 avg_loss:0.0071 total reward in epoch: [train:261.000 test:53.0] avg reward in episode:[train:3.382 test:2.944] e-greedy:0.900: 100%|██████████| 10000/10000 [05:22<00:00, 28.90it/s]\n",
      "epoch 072 avg_loss:0.0071 total reward in epoch: [train:269.000 test:54.0] avg reward in episode:[train:2.860 test:2.789] e-greedy:0.900: 100%|██████████| 10000/10000 [05:17<00:00, 31.48it/s]\n",
      "epoch 0073 loss 0.0148 rewards in epoch 4.000 episode 0000 rewards in episode 0.000.:   2%|▏         | 165/10000 [00:05<04:36, 35.59it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8f8bd384eaad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Docments/repos/ReNomRL/renom_rl/discrete/double_dqn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epoch, epoch_step, batch_size, random_step, test_step, update_period, train_frequency, min_greedy, max_greedy, greedy_step, test_greedy, render, callback_end_epoch)\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0mtrain_prestate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_terminal\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docments/ReNom/renom/layers/function/parameterized.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0muse_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_enter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_leave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docments/ReNom/renom/layers/function/parameterized.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mly\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docments/ReNom/renom/layers/function/parameterized.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0muse_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input must be at least of 2 dimensions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Issue 19330: ensure context manager instances have good docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@model.events.end_epoch\n",
    "def callback(*args):\n",
    "    pass\n",
    "\n",
    "model.fit(render=False, greedy_step=10000, random_step=50000, epoch_step=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network.save(\"dqn_exp5.h5\")\n",
    "# model = DQN(custom_env, q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7dc1cdbdf35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Docments/repos/ReNomRL/renom_rl/discrete/double_dqn.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, test_step, test_greedy, render)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0msum_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mreward_in_episode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-707bc2cedb37>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docments/repos/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docments/repos/gym/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docments/repos/gym/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/lib/python3.6/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch 0073 loss 0.0148 rewards in epoch 4.000 episode 0000 rewards in episode 0.000.:   2%|▏         | 166/10000 [00:20<04:36, 35.59it/s]"
     ]
    }
   ],
   "source": [
    "model.test(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_t = time.time()\n",
    "a = np.random.permutation(int(1e1))\n",
    "print(time.time()-start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
