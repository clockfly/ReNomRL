# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, grid
# This file is distributed under the same license as the ReNomRL package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: ReNomRL 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-10-02 09:18+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../rsts/continuous/renom_rl.continuous.ddpg.rst:2
msgid "renom_rl.continuous.ddpg"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:1
msgid "Bases: :class:`renom_rl.AgentBase`"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:1
msgid "DDPG class"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:3
msgid ""
"This class provides a reinforcement learning agent including training and"
" testing methods. This class only accepts 'Environment' as a object of "
"'BaseEnv' class."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG renom_rl.continuous.ddpg.DDPG.test
msgid "Parameters"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:6
msgid "An instance of Environment to be learned."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:8
msgid ""
"Actor-Network. If it is None, default ANN is created with [400, 300] "
"hidden layer sizes"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:11
msgid "basically a Q(s,a) function Network."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:13
msgid "Loss function for critic network. Default is MeanSquaredError()"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:14
msgid "Optimizer object for training actor network. Default is Adam(lr=0.0001)"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:15
msgid "Optimizer object for training actor network. Default is Adam(lr=0.001)"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:16
msgid "Discount rate."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:18
msgid ""
"target_networks update parameter. If this is 0, weight parameters will be"
" copied."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:20
msgid "The size of replay buffer."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG:30
msgid "Reference:"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG
msgid "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG
msgid "Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra,"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG
msgid "Continuous control with deep reinforcement learning"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG
msgid "https://arxiv.org/abs/1509.02971"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.action:1
msgid ""
"This method returns an action according to the given state. :param state:"
" A state of an environment. :type state: ndarray"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.action renom_rl.continuous.ddpg.DDPG.fit
#: renom_rl.continuous.ddpg.DDPG.target_value_function
#: renom_rl.continuous.ddpg.DDPG.test
#: renom_rl.continuous.ddpg.DDPG.value_function
msgid "Returns"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.action:5
msgid "Action."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.action renom_rl.continuous.ddpg.DDPG.fit
#: renom_rl.continuous.ddpg.DDPG.target_value_function
#: renom_rl.continuous.ddpg.DDPG.test
#: renom_rl.continuous.ddpg.DDPG.value_function
msgid "Return type"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.fit:1
msgid ""
"This method executes training of an actor-network. Here, target actor & "
"critic network weights are updated after every actor & critic update "
"using self.tau :param epoch: training number of epochs :type epoch: int "
":param epoch_step:"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.fit:6
msgid "Depends on the type of Environment in-built setting."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.fit:7
msgid "Environment reaches terminal situation in two cases."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.fit:8
msgid "In the type of Game, it is game over"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.fit:9
msgid "Maximum time steps to play"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.fit:12
msgid "A dictionary which includes reward list of training and loss list."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.value_function:1
msgid "Value of predict network Q_predict(s,a) :param state: input state"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.target_value_function:4
#: renom_rl.continuous.ddpg.DDPG.value_function:4
msgid "Q(s,a) value"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.target_value_function:1
msgid "Value of target network Q_target(s,a). :param state: input state"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.initalize:1
msgid ""
"target actor and critic networks are initialized with same neural network"
" weights as actor & critic network"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.update:1
msgid "updare target networks"
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.test:1
msgid "Test the trained agent."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.test:3
msgid ""
"Number of steps for test. If None is given, this method tests just 1 "
"episode."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.test:5
msgid "If True is given, BaseEnv.render() method will be called."
msgstr ""

#: of renom_rl.continuous.ddpg.DDPG.test:8
msgid "Sum of rewards."
msgstr ""

#~ msgid "Bases: :class:`object`"
#~ msgstr ""

#~ msgid ""
#~ "DDPG class Timothy P. Lillicrap, "
#~ "Jonathan J. Hunt, Alexander Pritzel, "
#~ "Nicolas Heess, Tom Erez, Yuval Tassa,"
#~ " David Silver, Daan Wierstra, Continuous"
#~ " control with deep reinforcement learning"
#~ " https://arxiv.org/abs/1509.02971"
#~ msgstr ""

#~ msgid ""
#~ "This class provides a reinforcement "
#~ "learning agent including training and "
#~ "testing methods. This class only accepts"
#~ " 'Environment' as a object of "
#~ "'BaseEnv' class or OpenAI-Gym 'Env' "
#~ "class."
#~ msgstr ""

#~ msgid ""
#~ "An instance of Environment to be "
#~ "learned. For example, 'Pendulum-v0' "
#~ "environment, has methods reset, step. "
#~ "env.reset() --> resets initial state of"
#~ " environment env.step(action) --> take "
#~ "action value and returns (next_state, "
#~ "reward, terminal, _)"
#~ msgstr ""

#~ msgid "Optimizer object for training actor network."
#~ msgstr ""

#~ msgid "Loss function for critic network."
#~ msgstr ""

#~ msgid "mini batch size."
#~ msgstr ""

#~ msgid ""
#~ "This method executes training of an "
#~ "actor-network. Here, target actor & "
#~ "critic network weights are updated after"
#~ " every actor & critic update using"
#~ " self.tau :param episode: training number"
#~ " of episodes :type episode: int "
#~ ":param episode_step: Depends on the type"
#~ " of Environment in-built setting."
#~ msgstr ""

#~ msgid ""
#~ "test the trained network Args: :returns:"
#~ " A list of cumulative test rewards"
#~ msgstr ""

#~ msgid ""
#~ "This method returns an action according"
#~ " to the given state. :param state:"
#~ " A state of an environment."
#~ msgstr ""

#~ msgid ""
#~ "This method executes training of an "
#~ "actor-network. Here, target actor & "
#~ "critic network weights are updated after"
#~ " every actor & critic update using"
#~ " self.tau :param epoch: training number "
#~ "of epochs :type epoch: int :param "
#~ "epoch_step: Depends on the type of "
#~ "Environment in-built setting."
#~ msgstr ""

