# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, grid
# This file is distributed under the same license as the ReNomRL package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
msgid ""
msgstr ""
"Project-Id-Version: ReNomRL 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-12-03 14:13+0900\n"
"PO-Revision-Date: 2018-11-05 09:45+0900\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.5.3\n"

#: ../../rsts/discrete/dqn.rst:2
msgid "renom_rl.discrete.dqn"
msgstr ""

#: of renom_rl.discrete.dqn.DQN:1
msgid "Bases: :class:`renom_rl.AgentBase`"
msgstr ""

#: of renom_rl.discrete.dqn.DQN:1
msgid ""
"DQN class This class provides a reinforcement learning agent including "
"training method."
msgstr "DQN クラスは学習アルゴリズムを含んだモデルです. "

#: of renom_rl.discrete.dqn.DQN:4
msgid "Environment. This must be a child class of ``BaseEnv``."
msgstr "エージェントを訓練させる環境. 環境オブジェクトはReNomRLが提供するBaseEnvクラスの子クラスである必要がある."

#: of renom_rl.discrete.dqn.DQN:6
msgid "Agent. Q-Network."
msgstr "エージェント. Q関数. "

#: of renom_rl.discrete.dqn.DQN:8
msgid ""
"Loss function for train q-network. Default is "
"``ClippedMeanSquaredError()``."
msgstr "Q-networkを訓練させるための誤差関数. デフォルトでは, ClippedMeanSquaredError()."

#: of renom_rl.discrete.dqn.DQN:10
msgid "Optimizer for train q-network. Default is ``Rmsprop(lr=0.00025, g=0.95)``."
msgstr "Q-networkを学習させるための勾配降下手法. デフォルトでは, Rmsprop(lr=0.00025, g=0.95)."

#: of renom_rl.discrete.dqn.DQN:11
msgid "Discount rate."
msgstr "割引率"

#: of renom_rl.discrete.dqn.DQN:13
msgid "The size of replay buffer."
msgstr "replay buffer のサイズ"

#: of renom_rl.discrete.dqn.DQN:18 renom_rl.discrete.dqn.DQN.fit:23
msgid "Example"
msgstr ""

#: of renom_rl.discrete.dqn.DQN:36
msgid "References"
msgstr ""

#: of renom_rl.discrete.dqn.DQN:37
msgid ""
"Volodymyr Mnih Koray Kavukcuoglu David Silver Alex Graves Ioannis "
"Antonoglou Daan Wierstra Martin Riedmille"
msgstr ""

#: of renom_rl.discrete.dqn.DQN:38
msgid "**Playing Atari with Deep Reinforcement Learning**"
msgstr ""

#: of renom_rl.discrete.dqn.DQN:39
msgid "https://arxiv.org/abs/1312.5602"
msgstr ""

#: of renom_rl.discrete.dqn.DQN.fit:1
msgid ""
"This method executes training of a q-network. Training will be done with "
"epsilon-greedy method(default)."
msgstr "Q-Networkの学習を実行する関数. 学習はe-greedy法に基づいて行われる."

#: of renom_rl.discrete.dqn.DQN.fit:4
msgid "Number of epoch for training."
msgstr "学習のためのエポック数"

#: of renom_rl.discrete.dqn.DQN.fit:6
msgid "Number of step of one epoch."
msgstr "１エポックにおけるステップ数"

#: of renom_rl.discrete.dqn.DQN.fit:8
msgid "Batch size."
msgstr "バッチサイズ. "

#: of renom_rl.discrete.dqn.DQN.fit:10
msgid "Number of random step which will be executed before training."
msgstr "乱数による行動選択. "

#: of renom_rl.discrete.dqn.DQN.fit:12
msgid "Number of test step."
msgstr "テストステップ数. ０の場合は, テストが終了するまで実行する. "

#: of renom_rl.discrete.dqn.DQN.fit:14
msgid "Period of updating target network."
msgstr "ターゲットネットワークを更新する頻度. "

#: of renom_rl.discrete.dqn.DQN.fit:16
msgid "For the learning step, training is done at this cycle"
msgstr "学習を実行する頻度. "

#: of renom_rl.discrete.dqn.DQN.fit:18
msgid "Exploration filter during learning. Default is `EpsilonGreedyFilter`."
msgstr "探索するためのアクションフィルタ. デフォルトは ``EpsilonGreedyFilter``. "

#: of renom_rl.discrete.dqn.DQN.test:1
msgid "Test the trained agent."
msgstr "学習したエージェントをテストする関数."

#: of renom_rl.discrete.dqn.DQN.test:3
msgid ""
"Number of steps (not episodes) for test. If None is given, this method "
"tests execute only 1 episode."
msgstr "テストの際に実行するステップ数.Noneが与えられた場合。エピソードが終了するまで実行する."

#: of renom_rl.discrete.dqn.DQN.test:5
msgid ""
"Exploartion filter during test. Default is "
"`ConstantFilter(threshold=1.0)`."
msgstr ""
"テスト時におけるアクションフィルタ. デフォルトは "
"`ConstantFilter(threshold=1.0)`."

#: of renom_rl.discrete.dqn.DQN.test:9
msgid "Sum of rewards."
msgstr "報酬の合計値"

#~ msgid "Parameters"
#~ msgstr ""

#~ msgid "Returns"
#~ msgstr "戻り値"

#~ msgid "Return type"
#~ msgstr "戻り値の型"

#~ msgid "Bases: :class:`renom_rl.AgentBase`"
#~ msgstr ""

#~ msgid ""
#~ "DQN class This class provides a "
#~ "reinforcement learning agent including "
#~ "training method."
#~ msgstr "DQN クラスは学習アルゴリズムを含んだモデルです. "

#~ msgid "Environment. This must be a child class of BaseEnv."
#~ msgstr "エージェントを訓練させる環境. 環境オブジェクトはReNomRLが提供するBaseEnvクラスの子クラスである必要がある."

#~ msgid "Q-Network."
#~ msgstr "エージェント. Q関数. "

#~ msgid ""
#~ "Loss function for train q-network. "
#~ "Default is ClippedMeanSquaredError()."
#~ msgstr "Q-networkを訓練させるための誤差関数. デフォルトでは, ClippedMeanSquaredError()."

#~ msgid "Optimizer for train q-network. Default is Rmsprop(lr=0.00025, g=0.95)."
#~ msgstr "Q-networkを学習させるための勾配降下手法. デフォルトでは, Rmsprop(lr=0.00025, g=0.95)."

#~ msgid "Discount rate."
#~ msgstr "割引率"

#~ msgid "The size of replay buffer."
#~ msgstr "replay buffer のサイズ"

#~ msgid "Example"
#~ msgstr ""

#~ msgid "References"
#~ msgstr ""

#~ msgid ""
#~ "Volodymyr Mnih Koray Kavukcuoglu David "
#~ "Silver Alex Graves Ioannis Antonoglou "
#~ "Daan Wierstra Martin Riedmille"
#~ msgstr ""

#~ msgid "**Playing Atari with Deep Reinforcement Learning**"
#~ msgstr ""

#~ msgid "https://arxiv.org/abs/1312.5602"
#~ msgstr ""

#~ msgid ""
#~ "Target q-network is initialized with "
#~ "same neural network weights of "
#~ "q-network."
#~ msgstr ""

#~ msgid "This method returns an action according to the given state."
#~ msgstr ""

#~ msgid "A state of an environment."
#~ msgstr ""

#~ msgid "Action."
#~ msgstr ""

#~ msgid "This function updates target network."
#~ msgstr ""

#~ msgid "This function updates best network in each target update period."
#~ msgstr ""

#~ msgid ""
#~ "This method executes training of a "
#~ "q-network. Training will be done with"
#~ " epsilon-greedy method."
#~ msgstr "Q-Networkの学習を実行する関数. 学習はe-greedy法に基づいて行われる."

#~ msgid "You can define following callback functions."
#~ msgstr ""

#~ msgid "- end_epoch"
#~ msgstr ""

#~ msgid "Args:"
#~ msgstr ""

#~ msgid "epoch (int): The number of current epoch."
#~ msgstr "エポック数"

#~ msgid "model (DQN): Object of DQN which is on training."
#~ msgstr "モデル"

#~ msgid ""
#~ "summed_train_reward_in_current_epoch (float): Sum of"
#~ " train rewards earned in current "
#~ "epoch."
#~ msgstr ""

#~ msgid "summed_test_reward_in_current_epoch (float): Sum of test rewards."
#~ msgstr ""

#~ msgid ""
#~ "average_train_loss_in_current_epoch (float): Average "
#~ "train loss in current epoch."
#~ msgstr ""

#~ msgid "Number of epoch for training."
#~ msgstr "学習のためのエポック数. "

#~ msgid "Number of step of one epoch."
#~ msgstr "１エポックにおけるステップ数. "

#~ msgid "Batch size."
#~ msgstr "バッチサイズ. "

#~ msgid "Number of random step which will be executed before training."
#~ msgstr "乱数による行動選択. "

#~ msgid "Number of test step."
#~ msgstr "テストステップ数. ０の場合は, テストが終了するまで実行する. "

#~ msgid "Period of updating target network."
#~ msgstr "ターゲットネットワークを更新する頻度. "

#~ msgid "For the learning step, training is done at this cycle"
#~ msgstr "学習を実行する頻度. "

#~ msgid "Minimum greedy value"
#~ msgstr "Greedy値の最小値."

#~ msgid "Maximum greedy value"
#~ msgstr "Greedy値の最大値."

#~ msgid "Number of step"
#~ msgstr "Greedyを最小から最大まで増加させるまでのstep数"

#~ msgid "Greedy threshold"
#~ msgstr "Test時に使用するGreedy値."

#~ msgid "If True is given, BaseEnv.render() method will be called in test time."
#~ msgstr "Trueが与えられた場合, BaseEnv.render()メソッドが実行される."

#~ msgid "Test the trained agent."
#~ msgstr "エージェントのテストを実行します. "

#~ msgid ""
#~ "Number of steps for test. If None"
#~ " is given, this method tests just "
#~ "1 episode."
#~ msgstr "Test時に実行するstep数. Noneが与えられた場合, 1episode分のTestが実行される."

#~ msgid "Greedy ratio of action."
#~ msgstr ""
#~ "与えられた割合(greedy)に応じて, 学習した価値関数, 方策に基づいて行動する. (1 "
#~ "- greedy)に応じて, 行動をランダムに選択する."

#~ msgid "If True is given, BaseEnv.render() method will be called."
#~ msgstr "Trueが与えられた時, BaseEnv.render()メソッドが呼ばれます."

#~ msgid "Sum of rewards."
#~ msgstr "報酬の合計値. "

#~ msgid "Environment. This must be a child class of ``BaseEnv``."
#~ msgstr "Evironmentです. ``BaseEnv`` を親クラスに持ちます. "

#~ msgid "Agent. Q-Network."
#~ msgstr "エージェント. Q関数. "

#~ msgid ""
#~ "Loss function for train q-network. "
#~ "Default is ``ClippedMeanSquaredError()``."
#~ msgstr "損失関数. デフォルトは ``ClippedMeanSquaredError()`` "

#~ msgid ""
#~ "Optimizer for train q-network. Default "
#~ "is ``Rmsprop(lr=0.00025, g=0.95)``."
#~ msgstr "Optimizer. デフォルトは ``Rmsprop(lr=0.00025, g=0.95)`` "

#~ msgid ""
#~ "This method executes training of a "
#~ "q-network. Training will be done with"
#~ " epsilon-greedy method(default)."
#~ msgstr "このメソッドは エージェント の学習を開始する. デフォルトでepsilon-greedy 法を用いて学習します. "

#~ msgid "Exploration filter during learning. Default is `EpsilonGreedyFilter`."
#~ msgstr "探索を実現する行動フィルタ. デフォルトは　``EpsilonGreedyFilter``. "

#~ msgid ""
#~ "Number of steps (not episodes) for "
#~ "test. If None is given, this "
#~ "method tests execute only 1 episode."
#~ msgstr "エージェントが実行するテストステップ数. ０の場合は, テストが終了するまで実行する. "

#~ msgid ""
#~ "Exploartion filter during test. Default "
#~ "is `ConstantFilter(threshold=1.0)`."
#~ msgstr "探索を実現する行動フィルタ. デフォルトは ``ConstantFilter(threshold=1.0)``. "
