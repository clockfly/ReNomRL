{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import renom as rm\n",
    "from renom.utility.initializer import Uniform, GlorotUniform\n",
    "from renom_rl.continuous.ddpg import DDPG\n",
    "from renom_rl.environ.openai import Pendulum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deifne the environment tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Pendulum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the actor network tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(rm.Model):\n",
    "    \n",
    "    '''Here considered 3-layer network (excluding input layer). Feel free to change the network depth \n",
    "    and check the results. The output-layer number of nuerons are equal to number of actions.\n",
    "    In the example of OpenAI Gym's Pendlum-v0 environment number of actions are 1.'''\n",
    "    \n",
    "    def __init__(self, layer_size):\n",
    "        self._layers = []\n",
    "        self.action_size = 1\n",
    "        self.high = 2.\n",
    "        self._l1 = rm.Dense(layer_size[0], initializer=GlorotUniform())\n",
    "        self._l2 = rm.Dense(layer_size[1], initializer=GlorotUniform())\n",
    "        self._l3 = rm.Dense(self.action_size, initializer=Uniform(min=-0.003, max=0.003))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Neural Network inputs are state information, outputs are actions. '''\n",
    "        h1 = rm.relu(self._l1(x))\n",
    "        h2 = rm.relu(self._l2(h1))\n",
    "        h3 = rm.tanh(self._l3(h2)) \n",
    "        h = h3*self.high\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Critic network tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(rm.Model):\n",
    "    '''Here considered a 3-layer network (input layer, hidden layer-1, hidden layer-2, output layer)\n",
    "        At input-layer state information, 2nd-hidden layer actions are applied, last layer has a single neuron'''\n",
    "    def __init__(self, layer_size):        \n",
    "        self._layers = []\n",
    "        self._l1 = rm.Dense(layer_size[0], initializer=GlorotUniform(), weight_decay=0.01)\n",
    "        self._l2 = rm.Dense(layer_size[1], initializer=GlorotUniform(), weight_decay=0.01)\n",
    "        self._l3 = rm.Dense(1, initializer=Uniform(min=-0.0003, max=0.0003), weight_decay=0.01)\n",
    "  \n",
    "    def forward(self, x, action):\n",
    "        '''Q(s,a) calculation for a given (state, action) pair'''\n",
    "        h1 = rm.relu(self._l1(x))\n",
    "        h2 = rm.relu(self._l2(rm.concat(h1, action))) # actions are applied at 2nd hidden layer\n",
    "        h = self._l3(h2)        \n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the actor & ciritic networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [400, 300] # two-hidden layers dimension\n",
    "actor_network = Actor(layer_size=layer_size)\n",
    "critic_network = Critic(layer_size=layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the DDPG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(env, actor_network, critic_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode 001 avg_loss: 4.103 total_reward [train:-1166.750 test:-] exploration:0.990: 100%|██████████| 200/200 [00:03<00:00, 56.09it/s]\n",
      "episode 002 avg_loss: 0.157 total_reward [train:-1386.071 test:-] exploration:0.980: 100%|██████████| 200/200 [00:06<00:00, 29.29it/s]\n",
      "episode 003 avg_loss: 0.064 total_reward [train:-1498.102 test:-] exploration:0.970: 100%|██████████| 200/200 [00:05<00:00, 34.24it/s]\n",
      "episode 004 avg_loss: 0.028 total_reward [train:-1256.605 test:-] exploration:0.960: 100%|██████████| 200/200 [00:04<00:00, 43.12it/s]\n",
      "episode 005 avg_loss: 0.024 total_reward [train:-1691.817 test:-] exploration:0.950: 100%|██████████| 200/200 [00:05<00:00, 35.23it/s]\n",
      "episode 006 avg_loss: 0.033 total_reward [train:-1910.309 test:-] exploration:0.940: 100%|██████████| 200/200 [00:05<00:00, 35.15it/s]\n",
      "episode 007 avg_loss: 0.045 total_reward [train:-1499.202 test:-] exploration:0.930: 100%|██████████| 200/200 [00:07<00:00, 25.69it/s]\n",
      "episode 008 avg_loss: 0.056 total_reward [train:-1627.860 test:-] exploration:0.920: 100%|██████████| 200/200 [00:04<00:00, 43.29it/s]\n",
      "episode 009 avg_loss: 0.133 total_reward [train:-1468.001 test:-] exploration:0.910: 100%|██████████| 200/200 [00:06<00:00, 27.84it/s]\n",
      "episode 010 avg_loss: 0.119 total_reward [train:-1502.997 test:-1440.619] exploration:0.900: 100%|██████████| 200/200 [00:06<00:00, 46.56it/s]\n",
      "episode 011 avg_loss: 0.183 total_reward [train:-1539.781 test:-] exploration:0.890: 100%|██████████| 200/200 [00:06<00:00, 30.45it/s]\n",
      "episode 012 avg_loss: 0.222 total_reward [train:-1575.626 test:-] exploration:0.880: 100%|██████████| 200/200 [00:03<00:00, 57.51it/s]\n",
      "episode 013 avg_loss: 0.268 total_reward [train:-1565.702 test:-] exploration:0.870: 100%|██████████| 200/200 [00:05<00:00, 33.41it/s]\n",
      "episode 014 avg_loss: 0.275 total_reward [train:-1567.608 test:-] exploration:0.860: 100%|██████████| 200/200 [00:04<00:00, 44.97it/s]\n",
      "episode 015 avg_loss: 0.390 total_reward [train:-1582.640 test:-] exploration:0.850: 100%|██████████| 200/200 [00:05<00:00, 35.18it/s]\n",
      "episode 016 avg_loss: 0.370 total_reward [train:-1508.811 test:-] exploration:0.840: 100%|██████████| 200/200 [00:05<00:00, 34.64it/s]\n",
      "episode 017 avg_loss: 0.522 total_reward [train:-1605.636 test:-] exploration:0.830: 100%|██████████| 200/200 [00:05<00:00, 33.93it/s]\n",
      "episode 018 avg_loss: 0.668 total_reward [train:-1489.041 test:-] exploration:0.820: 100%|██████████| 200/200 [00:06<00:00, 32.41it/s]\n",
      "episode 019 avg_loss: 0.674 total_reward [train:-1491.524 test:-] exploration:0.810: 100%|██████████| 200/200 [00:05<00:00, 34.38it/s]\n",
      "episode 020 avg_loss: 0.960 total_reward [train:-1511.434 test:-1490.456] exploration:0.800: 100%|██████████| 200/200 [00:05<00:00, 33.49it/s]\n",
      "episode 021 avg_loss: 1.313 total_reward [train:-1456.001 test:-] exploration:0.790: 100%|██████████| 200/200 [00:05<00:00, 37.49it/s]\n",
      "episode 022 avg_loss: 0.865 total_reward [train:-1531.773 test:-] exploration:0.780: 100%|██████████| 200/200 [00:06<00:00, 32.42it/s]\n",
      "episode 023 avg_loss: 1.200 total_reward [train:-1553.948 test:-] exploration:0.770: 100%|██████████| 200/200 [00:06<00:00, 28.12it/s]\n",
      "episode 024 avg_loss: 1.442 total_reward [train:-1569.409 test:-] exploration:0.760: 100%|██████████| 200/200 [00:05<00:00, 37.08it/s]\n",
      "episode 025 avg_loss: 1.532 total_reward [train:-1472.344 test:-] exploration:0.750: 100%|██████████| 200/200 [00:06<00:00, 27.83it/s]\n",
      "episode 026 avg_loss: 1.941 total_reward [train:-1407.269 test:-] exploration:0.740: 100%|██████████| 200/200 [00:05<00:00, 34.47it/s]\n",
      "episode 027 avg_loss: 1.217 total_reward [train:-1560.043 test:-] exploration:0.730: 100%|██████████| 200/200 [00:06<00:00, 32.80it/s]\n",
      "episode 028 avg_loss: 2.041 total_reward [train:-1052.736 test:-] exploration:0.720: 100%|██████████| 200/200 [00:05<00:00, 35.82it/s]\n",
      "episode 029 avg_loss: 2.258 total_reward [train:-1521.160 test:-] exploration:0.710: 100%|██████████| 200/200 [00:03<00:00, 50.79it/s]\n",
      "episode 030 avg_loss: 2.574 total_reward [train:-1448.842 test:-1303.310] exploration:0.700: 100%|██████████| 200/200 [00:05<00:00, 35.23it/s]\n",
      "episode 031 avg_loss: 2.059 total_reward [train:-1563.513 test:-] exploration:0.690: 100%|██████████| 200/200 [00:05<00:00, 33.41it/s]\n",
      "episode 032 avg_loss: 2.868 total_reward [train:-1567.099 test:-] exploration:0.680: 100%|██████████| 200/200 [00:06<00:00, 30.82it/s]\n",
      "episode 033 avg_loss: 3.215 total_reward [train:-1257.892 test:-] exploration:0.670: 100%|██████████| 200/200 [00:04<00:00, 42.04it/s]\n",
      "episode 034 avg_loss: 2.797 total_reward [train:-1205.096 test:-] exploration:0.660: 100%|██████████| 200/200 [00:03<00:00, 57.69it/s]\n",
      "episode 035 avg_loss: 3.915 total_reward [train:-1297.905 test:-] exploration:0.650: 100%|██████████| 200/200 [00:04<00:00, 49.04it/s]\n",
      "episode 036 avg_loss: 3.196 total_reward [train:-1199.971 test:-] exploration:0.640: 100%|██████████| 200/200 [00:05<00:00, 29.15it/s]\n",
      "episode 037 avg_loss: 3.975 total_reward [train:-1101.380 test:-] exploration:0.630: 100%|██████████| 200/200 [00:05<00:00, 35.73it/s]\n",
      "episode 038 avg_loss: 4.624 total_reward [train:-1105.375 test:-] exploration:0.620: 100%|██████████| 200/200 [00:05<00:00, 34.53it/s]\n",
      "episode 039 avg_loss: 3.514 total_reward [train:-1130.745 test:-] exploration:0.610: 100%|██████████| 200/200 [00:05<00:00, 36.69it/s]\n",
      "episode 040 avg_loss: 5.248 total_reward [train:-1074.636 test:-1061.335] exploration:0.600: 100%|██████████| 200/200 [00:05<00:00, 34.60it/s]\n",
      "episode 041 avg_loss: 3.794 total_reward [train:-1066.382 test:-] exploration:0.590: 100%|██████████| 200/200 [00:04<00:00, 58.57it/s]\n",
      "episode 042 avg_loss: 4.732 total_reward [train:-1062.027 test:-] exploration:0.580: 100%|██████████| 200/200 [00:03<00:00, 57.56it/s]\n",
      "episode 043 avg_loss: 5.331 total_reward [train:-1207.385 test:-] exploration:0.570: 100%|██████████| 200/200 [00:03<00:00, 56.59it/s]\n",
      "episode 044 avg_loss: 6.155 total_reward [train:-910.890 test:-] exploration:0.560: 100%|██████████| 200/200 [00:06<00:00, 33.16it/s]\n",
      "episode 045 avg_loss: 5.165 total_reward [train:-1022.792 test:-] exploration:0.550: 100%|██████████| 200/200 [00:05<00:00, 36.16it/s]\n",
      "episode 046 avg_loss: 6.041 total_reward [train:-955.433 test:-] exploration:0.540: 100%|██████████| 200/200 [00:04<00:00, 43.35it/s]\n",
      "episode 047 avg_loss: 5.856 total_reward [train:-819.097 test:-] exploration:0.530: 100%|██████████| 200/200 [00:03<00:00, 57.65it/s]\n",
      "episode 048 avg_loss: 7.781 total_reward [train:-873.862 test:-] exploration:0.520: 100%|██████████| 200/200 [00:03<00:00, 58.02it/s]\n",
      "episode 049 avg_loss: 6.999 total_reward [train:-994.004 test:-] exploration:0.510: 100%|██████████| 200/200 [00:06<00:00, 32.03it/s]\n",
      "episode 050 avg_loss: 8.313 total_reward [train:-853.058 test:-773.489] exploration:0.501: 100%|██████████| 200/200 [00:05<00:00, 34.59it/s]\n",
      "episode 051 avg_loss: 6.860 total_reward [train:-875.114 test:-] exploration:0.491: 100%|██████████| 200/200 [00:05<00:00, 35.83it/s]\n",
      "episode 052 avg_loss: 6.566 total_reward [train:-795.182 test:-] exploration:0.481: 100%|██████████| 200/200 [00:06<00:00, 40.54it/s]\n",
      "episode 053 avg_loss: 8.194 total_reward [train:-650.876 test:-] exploration:0.471: 100%|██████████| 200/200 [00:04<00:00, 41.49it/s]\n",
      "episode 054 avg_loss: 8.618 total_reward [train:-520.085 test:-] exploration:0.461: 100%|██████████| 200/200 [00:06<00:00, 33.07it/s]\n",
      "episode 055 avg_loss: 9.375 total_reward [train:-760.014 test:-] exploration:0.451: 100%|██████████| 200/200 [00:05<00:00, 39.28it/s]\n",
      "episode 056 avg_loss: 8.162 total_reward [train:-787.107 test:-] exploration:0.441: 100%|██████████| 200/200 [00:05<00:00, 37.09it/s]\n",
      "episode 057 avg_loss: 9.218 total_reward [train:-694.590 test:-] exploration:0.431: 100%|██████████| 200/200 [00:06<00:00, 32.85it/s]\n",
      "episode 058 avg_loss:10.870 total_reward [train:-665.879 test:-] exploration:0.421: 100%|██████████| 200/200 [00:06<00:00, 32.41it/s]\n",
      "episode 059 avg_loss: 7.211 total_reward [train:-526.412 test:-] exploration:0.411: 100%|██████████| 200/200 [00:06<00:00, 31.57it/s]\n",
      "episode 060 avg_loss:10.255 total_reward [train:-501.344 test:-397.945] exploration:0.401: 100%|██████████| 200/200 [00:06<00:00, 35.65it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode 061 avg_loss:10.126 total_reward [train:-394.436 test:-] exploration:0.391: 100%|██████████| 200/200 [00:06<00:00, 27.70it/s]\n",
      "episode 062 avg_loss:13.227 total_reward [train:-567.400 test:-] exploration:0.381: 100%|██████████| 200/200 [00:05<00:00, 37.55it/s]\n",
      "episode 063 avg_loss: 8.321 total_reward [train:-396.226 test:-] exploration:0.371: 100%|██████████| 200/200 [00:05<00:00, 38.47it/s]\n",
      "episode 064 avg_loss: 9.313 total_reward [train:-641.517 test:-] exploration:0.361: 100%|██████████| 200/200 [00:05<00:00, 34.58it/s]\n",
      "episode 065 avg_loss: 9.572 total_reward [train:-538.943 test:-] exploration:0.351: 100%|██████████| 200/200 [00:06<00:00, 33.01it/s]\n",
      "episode 066 avg_loss: 7.657 total_reward [train:-395.497 test:-] exploration:0.341: 100%|██████████| 200/200 [00:06<00:00, 31.69it/s]\n",
      "episode 067 avg_loss:12.014 total_reward [train:-499.647 test:-] exploration:0.331: 100%|██████████| 200/200 [00:05<00:00, 28.53it/s]\n",
      "episode 068 avg_loss:12.024 total_reward [train:-396.999 test:-] exploration:0.321: 100%|██████████| 200/200 [00:06<00:00, 32.13it/s]\n",
      "episode 069 avg_loss: 7.195 total_reward [train:-412.355 test:-] exploration:0.311: 100%|██████████| 200/200 [00:05<00:00, 34.48it/s]\n",
      "episode 070 avg_loss:11.137 total_reward [train:-146.162 test:-144.780] exploration:0.301: 100%|██████████| 200/200 [00:06<00:00, 32.86it/s]\n",
      "episode 071 avg_loss:12.367 total_reward [train:-381.453 test:-] exploration:0.291: 100%|██████████| 200/200 [00:04<00:00, 42.82it/s]\n",
      "episode 072 avg_loss: 8.256 total_reward [train:-264.824 test:-] exploration:0.281: 100%|██████████| 200/200 [00:06<00:00, 33.17it/s]\n",
      "episode 073 avg_loss:11.684 total_reward [train:-138.318 test:-] exploration:0.271: 100%|██████████| 200/200 [00:06<00:00, 34.54it/s]\n",
      "episode 074 avg_loss:13.159 total_reward [train:-400.969 test:-] exploration:0.261: 100%|██████████| 200/200 [00:06<00:00, 32.51it/s]\n",
      "episode 075 avg_loss:10.970 total_reward [train:-261.406 test:-] exploration:0.251: 100%|██████████| 200/200 [00:05<00:00, 35.99it/s]\n",
      "episode 076 avg_loss:12.327 total_reward [train:-398.854 test:-] exploration:0.241: 100%|██████████| 200/200 [00:05<00:00, 35.85it/s]\n",
      "episode 077 avg_loss:13.871 total_reward [train:-267.565 test:-] exploration:0.231: 100%|██████████| 200/200 [00:04<00:00, 40.48it/s]\n",
      "episode 078 avg_loss:12.159 total_reward [train:-267.625 test:-] exploration:0.221: 100%|██████████| 200/200 [00:03<00:00, 58.22it/s]\n",
      "episode 079 avg_loss:10.795 total_reward [train:-391.264 test:-] exploration:0.211: 100%|██████████| 200/200 [00:05<00:00, 33.91it/s]\n",
      "episode 080 avg_loss:15.450 total_reward [train:-263.768 test:-144.757] exploration:0.201: 100%|██████████| 200/200 [00:06<00:00, 28.63it/s]\n",
      "episode 081 avg_loss:13.711 total_reward [train:-270.994 test:-] exploration:0.191: 100%|██████████| 200/200 [00:05<00:00, 37.30it/s]\n",
      "episode 082 avg_loss:15.844 total_reward [train:-267.448 test:-] exploration:0.181: 100%|██████████| 200/200 [00:05<00:00, 30.87it/s]\n",
      "episode 083 avg_loss:17.489 total_reward [train:-265.108 test:-] exploration:0.171: 100%|██████████| 200/200 [00:06<00:00, 30.42it/s]\n",
      "episode 084 avg_loss:10.713 total_reward [train:-144.565 test:-] exploration:0.161: 100%|██████████| 200/200 [00:05<00:00, 39.07it/s]\n",
      "episode 085 avg_loss:11.197 total_reward [train:-269.145 test:-] exploration:0.151: 100%|██████████| 200/200 [00:06<00:00, 33.32it/s]\n",
      "episode 086 avg_loss:12.251 total_reward [train:-384.392 test:-] exploration:0.141: 100%|██████████| 200/200 [00:05<00:00, 36.47it/s]\n",
      "episode 087 avg_loss:12.766 total_reward [train:-259.436 test:-] exploration:0.131: 100%|██████████| 200/200 [00:05<00:00, 36.56it/s]\n",
      "episode 088 avg_loss:17.368 total_reward [train:-491.088 test:-] exploration:0.121: 100%|██████████| 200/200 [00:06<00:00, 32.60it/s]\n",
      "episode 089 avg_loss:15.373 total_reward [train:-280.594 test:-] exploration:0.111: 100%|██████████| 200/200 [00:05<00:00, 34.54it/s]\n",
      "episode 090 avg_loss:14.517 total_reward [train:-265.566 test:-390.912] exploration:0.101: 100%|██████████| 200/200 [00:06<00:00, 32.14it/s]\n",
      "episode 091 avg_loss:14.723 total_reward [train:-321.976 test:-] exploration:0.091: 100%|██████████| 200/200 [00:06<00:00, 32.04it/s]\n",
      "episode 092 avg_loss:12.726 total_reward [train:-474.220 test:-] exploration:0.081: 100%|██████████| 200/200 [00:05<00:00, 36.91it/s]\n",
      "episode 093 avg_loss:17.433 total_reward [train:-417.304 test:-] exploration:0.071: 100%|██████████| 200/200 [00:05<00:00, 33.86it/s]\n",
      "episode 094 avg_loss:12.080 total_reward [train:-259.437 test:-] exploration:0.061: 100%|██████████| 200/200 [00:06<00:00, 32.64it/s]\n",
      "episode 095 avg_loss:13.504 total_reward [train:-149.030 test:-] exploration:0.051: 100%|██████████| 200/200 [00:04<00:00, 41.67it/s]\n",
      "episode 096 avg_loss:14.454 total_reward [train:-269.815 test:-] exploration:0.041: 100%|██████████| 200/200 [00:06<00:00, 32.66it/s]\n",
      "episode 097 avg_loss:16.082 total_reward [train:-146.515 test:-] exploration:0.031: 100%|██████████| 200/200 [00:06<00:00, 30.28it/s]\n",
      "episode 098 avg_loss:12.054 total_reward [train:-264.816 test:-] exploration:0.021: 100%|██████████| 200/200 [00:05<00:00, 34.37it/s]\n",
      "episode 099 avg_loss:20.356 total_reward [train:-143.900 test:-] exploration:0.011: 100%|██████████| 200/200 [00:06<00:00, 33.23it/s]\n",
      "episode 100 avg_loss:20.731 total_reward [train:-270.485 test:-389.156] exploration:0.001: 100%|██████████| 200/200 [00:05<00:00, 35.20it/s]\n"
     ]
    }
   ],
   "source": [
    "ddpg.fit(episode=100, episode_step=200, exploration_step=20000, min_exploration_rate=0.001, max_exploration_rate=1.0, test_step=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1659.56423196\n"
     ]
    }
   ],
   "source": [
    "test_reward_list = ddpg.test(render=False)\n",
    "print(test_reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(test_reward_list)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Rewards per Episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
