{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Proof of A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import renom as rm\n",
    "from renom.optimizer import Adam\n",
    "from renom_rl.discrete.a2c import A2C\n",
    "from renom_rl.environ.env import BaseEnv\n",
    "from tqdm import tqdm\n",
    "from renom_rl.utility import Animation\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from renom_rl.utility.additional_modules import Orthogonal\n",
    "from renom.cuda import set_cuda_active\n",
    "from renom_rl.utility.logger import Logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set_cuda_active(True)\n",
    "\n",
    "class ActorCritic(rm.Model):\n",
    "    def __init__(self):\n",
    "        self.c1=rm.Conv2d(32, filter=8, padding=0, stride=4, initializer=Orthogonal(1.412))\n",
    "        self.c2=rm.Conv2d(64, filter=4, padding=0, stride=2, initializer=Orthogonal(1.412))\n",
    "        self.c3=rm.Conv2d(64, filter=3, padding=0, stride=1, initializer=Orthogonal(1.412))\n",
    "        self.l1=rm.Dense(512, initializer=Orthogonal(1.412))\n",
    "        self.l3=rm.Dense(4, initializer=Orthogonal(gain=0.01))\n",
    "        self.l4=rm.Dense(1, initializer=Orthogonal(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h = self.c1(x)\n",
    "        h = rm.relu(h)\n",
    "        h = self.c2(h)\n",
    "        h = rm.relu(h)\n",
    "        h = self.c3(h)\n",
    "        h = rm.relu(h)        \n",
    "        h = rm.flatten(h)\n",
    "        h = self.l1(h)\n",
    "        h = rm.relu(h)\n",
    "        act = rm.softmax(self.l3(h))\n",
    "        val=self.l4(h)\n",
    "        return act,val\n",
    "    \n",
    "model=ActorCritic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from gym import spaces\n",
    "from gym.spaces.box import Box\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        '''工夫1のNo-Operationです。リセット後適当なステップの間何もしないようにし、\n",
    "        ゲーム開始の初期状態を様々にすることｆで、特定の開始状態のみで学習するのを防ぐ'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(\n",
    "                1, self.noop_max + 1)  # pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        _=self.env.step(1)\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        '''工夫2のEpisodic Lifeです。1機失敗したときにリセットし、失敗時の状態から次を始める'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        '''5機とも失敗したら、本当にリセット'''\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        '''工夫3のMax and Skipです。4フレーム連続で同じ行動を実施し、最後の3、4フレームの最大値をとった画像をobsにする'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros(\n",
    "            (2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        '''工夫4のWarp frameです。画像サイズをNatureのDQN論文と同じ84x84の白黒にします'''\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height),\n",
    "                           interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        '''PyTorchのミニバッチのインデックス順に変更するラッパー'''\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0],\n",
    "            [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, rank):\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env.seed(seed + rank)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    env = WrapPyTorch(env)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Breakout(BaseEnv):\n",
    "    \n",
    "    def __init__(self,seed=1,i=1):\n",
    "        self.env = make_env('BreakoutNoFrameskip-v4',seed,i)\n",
    "        self.action_shape = (self.env.action_space.n,)\n",
    "        self.action_interval = 4\n",
    "        self.state_shape = (4, 84, 84)\n",
    "        self.test=False\n",
    "        self.lives = 5\n",
    "        self.true_terminal = True\n",
    "        self.animation = Animation(ratio=36.0)\n",
    "        self.test_mode = False\n",
    "        self.previous_frames=[np.zeros((84,84))]*4\n",
    "        \n",
    "    def reset(self): \n",
    "        \n",
    "        self.previous_frames=[np.zeros((84,84))]*4\n",
    "        self.append_and_get(self.env.reset())\n",
    "        \n",
    "        return np.stack(self.previous_frames) \n",
    "    \n",
    "    def sample(self):\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "\n",
    "        \n",
    "    def append_and_get(self,state):\n",
    "\n",
    "        state = state.squeeze().copy()/255\n",
    "        \n",
    "        if len(self.previous_frames) > 3: \n",
    "            self.previous_frames = self.previous_frames[1:] + [state]\n",
    "        else:\n",
    "            self.previous_frames += [state]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        state, reward, terminal, _ = self.env.step(action)    \n",
    "\n",
    "        self.append_and_get(state)\n",
    "        \n",
    "        state_final = np.stack(self.previous_frames)            \n",
    "\n",
    "        return state_final, reward, terminal\n",
    "    \n",
    "\n",
    "    def epoch(self):\n",
    "        self.true_terminal = True\n",
    "    \n",
    "    def test_start(self):\n",
    "        self.true_terminal = True\n",
    "#         model2.save(\"model1.h5\")\n",
    "        if self.test:\n",
    "            self.animation.reset()\n",
    "            self.env.reset()\n",
    "    \n",
    "    def test_epoch_step(self):\n",
    "        if self.test:\n",
    "            self.animation.store(self.env.render(mode=\"rgb_array\"))\n",
    "    \n",
    "    def test_close(self):\n",
    "        #self.env.close() \n",
    "        if self.test:\n",
    "            self.env.viewer = None\n",
    "    \n",
    "env = Breakout()\n",
    "_=env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class OriginalLogger(Logger):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(record = False)\n",
    "\n",
    "        self.episodes = 0\n",
    "        self.total_reward=[0]\n",
    "        self.tick = 0\n",
    "        self.episode_lifes = 0\n",
    "        self.episode_lifes_constant = 5\n",
    "        self.terminal_list=[]\n",
    "        self.episode_rewards=0\n",
    "        self.died = 0\n",
    "        \n",
    "        self.max = 0\n",
    "        self.entropy = 0\n",
    "        \n",
    "        \n",
    "        self.total_loss_list=[]\n",
    "        self.loss_list=[]\n",
    "        self.entropy_list=[]\n",
    "        self.reward_list=[]\n",
    "        self.max_reward_list=[]\n",
    "\n",
    "    \n",
    "    def logger(self, **log):\n",
    "\n",
    "        # reset\n",
    "        if not log[\"epoch_step\"][0]:\n",
    "            self.episodes = 0\n",
    "            self.total_reward=[0]\n",
    "            self.tick = 0\n",
    "            self.episode_lifes = 5\n",
    "            self.terminal_list=[]\n",
    "            self.episode_rewards = 0\n",
    "            self.died = 0\n",
    "            \n",
    "            self.max=0\n",
    "\n",
    "        #　defining variable\n",
    "        e = log[\"epoch\"][-1]\n",
    "        loss = log[\"loss\"][-1]\n",
    "        terminal_index = np.where(log[\"terminal\"].squeeze())\n",
    "        \n",
    "\n",
    "        #　for epoch calc\n",
    "        if log[\"terminal\"].any():\n",
    "            self.tick += len(log[\"terminal\"][terminal_index])\n",
    "            self.died += len(log[\"terminal\"][terminal_index])\n",
    "            self.terminal_list += log[\"sum_reward\"][terminal_index].reshape(-1,1).tolist()\n",
    "\n",
    "        \n",
    "        if self.died // self.episode_lifes_constant > 0:\n",
    "            \n",
    "            self.episodes += self.died//self.episode_lifes_constant\n",
    "            total_reward_index_end = self.episode_lifes_constant*(self.tick//self.episode_lifes_constant)\n",
    "            \n",
    "            total_reward_index_start = self.episode_lifes_constant*((self.tick-1)//self.episode_lifes_constant)\n",
    "            \n",
    "            self.total_reward.append(float(np.sum(np.array(self.terminal_list[total_reward_index_start:total_reward_index_end]))))\n",
    "            \n",
    "            self.died -= 5*(self.died//self.episode_lifes_constant)\n",
    "            self.episode_lifes += 5*(self.died//self.episode_lifes_constant)\n",
    "            \n",
    "            \n",
    "        loss = log[\"loss\"][-1]\n",
    "        entropy = log[\"entropy\"][-1]\n",
    "        self.entropy = entropy\n",
    "        \n",
    "        if self.total_reward[-1] > self.max:\n",
    "            self.max = self.total_reward[-1]\n",
    "\n",
    "        max_val = self.max\n",
    "        \n",
    "        msg = \"agent[0], entropy:{:0=+1.4f}, epoch {:04d}, loss {:5.4f}, epoch reward total {:4.3f},  episode:{:04.1f}, rewards: {:4.3f}, max reward: {}\"\\\n",
    "            .format(entropy, e, loss, float(np.sum(self.total_reward)), self.episodes, self.total_reward[-1], max_val)\n",
    "        \n",
    "        self.total_loss_list.append(log[\"total_loss\"][-1])\n",
    "        self.loss_list.append(loss)\n",
    "        self.entropy_list.append(entropy)\n",
    "        self.reward_list.append(int(np.array(self.terminal_list[-1])) if len(self.terminal_list) else 0)\n",
    "        self.max_reward_list.append(max_val)\n",
    "        \n",
    "        return msg\n",
    "\n",
    "    \n",
    "\n",
    "    def logger_epoch(self, **log):\n",
    "        e = log[\"epoch\"]\n",
    "        entropy = self.entropy\n",
    "        avg_train_reward = np.sum(self.total_reward)/(self.tick + 1)\n",
    "        train_reward = np.sum(self.total_reward)\n",
    "        test_reward = log[\"test_reward\"]\n",
    "\n",
    "        msg = \"epoch {:03d} total reward in epoch: [train:{:4.3f} test:{:4.3}] \" + \\\n",
    "            \"avg train reward per episode:{:4.3f}, max rewad: {}, entropy: {:0=+1.4f}\"\n",
    "        msg = msg.format(e, train_reward,\n",
    "                         test_reward, avg_train_reward, self.max, entropy)\n",
    "\n",
    "        return msg\n",
    "    \n",
    "logger = OriginalLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# blank_list=[]\n",
    "# for i in np.random.randint(0,4,10):\n",
    "#     s,r,w=env.step(i)\n",
    "#     blank_list.append(np.concatenate(s,1))\n",
    "\n",
    "# res=np.stack(blank_list)\n",
    "# print(res.shape)\n",
    "# res=np.concatenate(res,axis=0)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "\n",
    "# plt.imshow(res)\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Env=Breakout()\n",
    "opt=Adam(lr=0.01)\n",
    "from renom_rl.utility import ProbNodeChooser\n",
    "from renom_rl.utility import GradientClipping\n",
    "\n",
    "a2c=A2C(Env,\n",
    "        model,\n",
    "        gradient_clipping=GradientClipping(),\n",
    "        num_worker=16,\n",
    "        logger=logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a2c.fit(\n",
    "    epoch=3,\n",
    "    epoch_step=30000,\n",
    "    test_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Env.test=True\n",
    "a2c.test(1000)\n",
    "Env.test=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Env.animation.run()\n",
    "Env.animation.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.loss_list=(np.array(logger.loss_list)*2).tolist()\n",
    "logger.reward_list=np.array([int(np.array(i)) for i in logger.reward_list]).squeeze().tolist()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params=[\"total_loss\",\"entropy\",\"loss\",\"reward\",\"max_reward\"]\n",
    "params=[i+\"_list\" for i in params]\n",
    "# fig=plt.figure()\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i , p in enumerate(params,1):\n",
    "    plt.subplot(len(params),1, i)\n",
    "    logger.graph_attribute(plt,getattr(logger,p),y_label=p,x_label=\"update\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
