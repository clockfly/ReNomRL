{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import renom as rm\n",
    "from renom.utility.initializer import Uniform, GlorotUniform\n",
    "from renom_rl.continuous.a3c_2 import A3C\n",
    "from renom_rl.env import BaseEnv\n",
    "from renom.utility.initializer import Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deifne the environment tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(1,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "print(env.action_space.shape)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "class CustomEnv(BaseEnv):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_shape = (1, )\n",
    "        self.state_shape = (3, )\n",
    "        super(CustomEnv, self).__init__()\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)[:3]\n",
    "\n",
    "custom_env = CustomEnv(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the actor network tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the actor & ciritic networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC(rm.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        bias = False\n",
    "        self._dn1 = rm.Dense(100, ignore_bias=bias)\n",
    "        self._dn2 = rm.Dense(100, ignore_bias=bias)\n",
    "        self._dn_a = rm.Dense(2, ignore_bias=bias)\n",
    "        self._dn_c = rm.Dense(1, ignore_bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = rm.relu(self._dn1(x))\n",
    "        h = rm.relu(self._dn2(h))\n",
    "        p = self._dn_a(h)\n",
    "        v = self._dn_c(h)\n",
    "        return p, v\n",
    "    \n",
    "nn_model = AC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the DDPG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = A3C(custom_env, nn_model, loss_func=rm.mse, num_worker=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0000 Average Train reward: -1292.000 Test reward: -1496.317: : 80it [01:33,  1.17s/it]\n",
      "0001 Average Train reward: -1279.227 Test reward: -824.983: : 80it [01:34,  3.40it/s]\n",
      "0002 Average Train reward: -1033.457 Test reward: -1491.098: : 80it [01:35,  1.19s/it]\n",
      "0003 Average Train reward: -1419.982 Test reward: -1388.784: : 80it [01:35,  1.19s/it]\n",
      "0004 Average Train reward: -1274.359 Test reward: -1548.332: : 80it [01:33,  1.17s/it]\n",
      "0005 Average Train reward: -1164.958 Test reward: -1489.267: : 80it [01:33,  2.48it/s]\n",
      "0006 Average Train reward: -1165.232 Test reward: -1483.449: : 80it [01:34,  4.11it/s]\n",
      "0007 Average Train reward: -1075.862 Test reward: -1478.427: : 80it [01:34,  1.18s/it]\n",
      "0008 Average Train reward: -1284.431 Test reward: -1651.784: : 80it [01:33,  3.94it/s]\n",
      "0009 Average Train reward: -1394.216 Test reward: -1648.264: : 80it [01:33,  2.61it/s]\n",
      "0010 Average Train reward: -1054.183 Test reward: -1127.873: : 80it [01:33,  1.17s/it]\n",
      "0011 Average Train reward: -1251.479 Test reward: -1500.040: : 80it [01:34,  1.18s/it]\n",
      "0012 Average Train reward: -1389.694 Test reward: -1366.246: : 80it [01:35,  5.04it/s]\n",
      "0013 Average Train reward: -1097.890 Test reward: -1647.248: : 80it [01:35,  2.39it/s]\n",
      "0014 Average Train reward: -1471.263 Test reward: -1011.504: : 80it [01:34,  2.85it/s]\n",
      "0015 Average Train reward: -1036.779 Test reward: -1614.286: : 80it [01:34,  1.18s/it]\n",
      "0016 Average Train reward: -1302.588 Test reward: -1367.324: : 80it [01:35,  2.52it/s]\n",
      "0017 Average Train reward: -1420.620 Test reward: -1508.258: : 80it [01:33,  1.97it/s]\n",
      "0018 Average Train reward: -1466.534 Test reward: -1493.747: : 80it [01:35,  1.19s/it]\n",
      "0019 Average Train reward: -1183.499 Test reward: -1620.315: : 80it [01:34,  1.18s/it]\n",
      "0020 Average Train reward: -1292.102 Test reward: -1475.781: : 80it [01:35,  2.65it/s]\n",
      "0021 Average Train reward: -1258.523 Test reward: -1464.255: : 80it [01:34,  3.16it/s]\n",
      "0022 Average Train reward: -1332.071 Test reward: -1652.907: : 80it [01:35,  4.33it/s]\n",
      "0023 Average Train reward: -1222.070 Test reward: -1265.671: : 80it [01:35,  1.75it/s]\n",
      "0024 Average Train reward: -1346.758 Test reward: -1339.801: : 80it [01:35,  3.94it/s]\n",
      "0025 Average Train reward: -1187.458 Test reward: -1125.504: : 80it [01:34,  2.55it/s]\n",
      "0026 Average Train reward: -1324.443 Test reward: -1507.974: : 80it [01:33,  3.39it/s]\n",
      "0027 Average Train reward: -1326.377 Test reward: -1627.870: : 80it [01:36,  1.20s/it]\n",
      "0028 Average Train reward: -1147.825 Test reward: -1086.346: : 80it [01:35,  1.19s/it]\n",
      "0029 Average Train reward: -1318.687 Test reward: -1648.617: : 80it [01:35,  3.17it/s]\n",
      "0030 Average Train reward: -1290.022 Test reward: -956.687: : 80it [01:35,  1.84it/s]\n",
      "0031 Average Train reward: -1228.266 Test reward: -1629.735: : 80it [01:33,  2.06it/s]\n",
      "0032 Average Train reward: -1144.817 Test reward: -860.383: : 80it [01:33,  2.00it/s]\n",
      "0033 Average Train reward: -1222.010 Test reward: -1041.783: : 80it [01:33,  3.04it/s]\n",
      "0034 Average Train reward: -1199.861 Test reward: -1656.004: : 80it [01:33,  1.85it/s]\n",
      "0035 Average Train reward: -1103.869 Test reward: -1655.308: : 80it [01:36,  2.44it/s]\n",
      "0036 Average Train reward: -1231.327 Test reward: -1499.247: : 80it [01:36,  2.73it/s]\n",
      "0037 Average Train reward: -1170.769 Test reward: -1655.096: : 80it [01:34,  1.18s/it]\n",
      "0038 Average Train reward: -1227.516 Test reward: -1377.595: : 80it [01:35,  3.09it/s]\n",
      "0039 Average Train reward: -1167.949 Test reward: -1654.443: : 80it [01:35,  1.20s/it]\n",
      "0040 Average Train reward: -1235.819 Test reward: -1146.132: : 80it [01:37,  4.30it/s]\n",
      "0041 Average Train reward: -1209.507 Test reward: -948.423: : 80it [01:37,  1.21s/it]\n",
      "0042 Average Train reward: -1139.420 Test reward: -1646.211: : 80it [01:37,  3.87it/s]\n",
      "0043 Average Train reward: -1214.422 Test reward: -1628.136: : 80it [01:35,  6.41it/s]\n",
      "0044 Average Train reward: -1180.260 Test reward: -1135.939: : 80it [01:36,  2.85it/s]\n",
      "0045 Average Train reward: -1216.411 Test reward: -1062.901: : 80it [01:35,  2.40it/s]\n",
      "0046 Average Train reward: -1297.948 Test reward: -1496.445: : 80it [01:34,  3.16it/s]\n",
      "0047 Average Train reward: -1145.826 Test reward: -1288.067: : 80it [01:33,  2.04it/s]\n",
      "0048 Average Train reward: -1240.005 Test reward: -732.944: : 80it [01:32,  3.84it/s]\n",
      "0049 Average Train reward: -1095.914 Test reward: -1495.747: : 80it [01:34,  2.46it/s]\n",
      "0050 Average Train reward: -1191.086 Test reward: -1654.148: : 80it [01:34,  1.18s/it]\n",
      "0051 Average Train reward: -1309.169 Test reward: -1496.676: : 80it [01:33,  1.17s/it]\n",
      "0052 Average Train reward: -1127.704 Test reward: -1090.889: : 80it [01:34,  1.18s/it]\n",
      "0053 Average Train reward: -1242.722 Test reward: -1649.562: : 80it [01:34,  1.18s/it]\n",
      "0054 Average Train reward: -1161.023 Test reward: -947.242: : 80it [01:34,  3.43it/s]\n",
      "0055 Average Train reward: -1144.872 Test reward: -1646.216: : 80it [01:34,  1.18s/it]\n",
      "0056 Average Train reward: -1141.278 Test reward: -1175.356: : 80it [01:34,  3.32it/s]\n",
      "0057 Average Train reward: -1365.070 Test reward: -1657.818: : 80it [01:33,  3.06it/s]\n",
      "0058 Average Train reward: -1158.027 Test reward: -1322.303: : 80it [01:34,  3.12it/s]\n",
      "0059 Average Train reward: -1310.513 Test reward: -985.706: : 80it [01:32,  1.87it/s]\n",
      "0060 Average Train reward: -1231.227 Test reward: -1648.877: : 80it [01:32,  2.48it/s]\n",
      "17it [00:33,  4.73s/it]/home/ubuntu/Documents/local_repository/ReNom/renom/optimizer.py:221: RuntimeWarning: overflow encountered in square\n",
      "  r = self._g * pdy + (1 - self._g) * (dy**2)\n",
      "0061 Average Train reward: -1156.807 Test reward: -1588.755: : 80it [01:33,  3.41it/s]\n",
      "0062 Average Train reward: -1393.020 Test reward: -1494.072: : 80it [01:34,  3.86it/s]\n",
      "0063 Average Train reward: -1230.137 Test reward: -1343.438: : 80it [01:33,  1.16s/it]\n",
      "0064 Average Train reward: -1156.408 Test reward: -1355.106: : 80it [01:33,  3.30it/s]\n",
      "64it [01:16,  2.75it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ee733152f41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/local_repository/ReNomRL/renom_rl/continuous/a3c_2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, episode, episode_step, min_greedy, max_greedy, greedy_step, test_step, test_frequency, render, callback_end_epoch)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_worker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slave_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mnext_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [01:32,  1.83s/it]"
     ]
    }
   ],
   "source": [
    "ddpg.fit(episode=10000, episode_step=2000, test_frequency=16*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward_list = ddpg.test(render=True)\n",
    "print(test_reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(test_reward_list)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Rewards per Episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
