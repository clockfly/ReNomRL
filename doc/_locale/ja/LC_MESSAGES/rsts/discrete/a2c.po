# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, grid
# This file is distributed under the same license as the ReNomRL package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: ReNomRL 0.2\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-01-29 19:31+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.5.3\n"

#: ../../rsts/discrete/a2c.rst:2
msgid "renom_rl.discrete.a2c"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:1
msgid "Bases: :class:`renom_rl.AgentBase`"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:3
msgid "A2C class"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:2
msgid ""
"This class provides a reinforcement learning agent including training "
"method. This class runs on a single thread."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:5
msgid "Environment. This must be a child class of ``BaseEnv``."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:7
msgid "Actor Critic Model."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:9
msgid "Number of actor/environment model."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:11
msgid "Advantage steps."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:13
msgid "node selector."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:15
msgid "test node selector."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:17
msgid "Loss function for train q-network. Default is ``MeanSquaredError()``."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:19
msgid "Optimizer for train q-network. Default is ``Rmsprop(lr=0.00025, g=0.95)``."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:20
msgid "Coefficient of actor's output entropy."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:22
msgid "Coefficient of value loss."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:24
msgid "Discount rate."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:26
msgid "The size of replay buffer."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:31
msgid "Example"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:57
msgid "References"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:58
msgid "A. V. Clemente, H. N. Castejon, and A. Chandra."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:59
msgid "Efficient Parallel Methods for Deep Reinforcement Learning"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C:60
msgid "https://arxiv.org/abs/1705.04862"
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.fit:1
msgid ""
"This method executes training of actor critic. Test will be runned after "
"each epoch is done."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.fit:4
msgid "Number of epoch for training."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.fit:6
msgid "Number of step of one epoch."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.fit:8
msgid "Number steps during test."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.test:1
msgid "Test the trained actor agent."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.test:3
msgid ""
"Number of steps (not episodes) for test. If None is given, this method "
"tests execute only 1 episode."
msgstr ""

#: ../../docstring of renom_rl.discrete.a2c.A2C.test:7
msgid "Sum of rewards."
msgstr ""

#~ msgid "Bases: :class:`object`"
#~ msgstr ""

#~ msgid "A2C class"
#~ msgstr ""

#~ msgid ""
#~ "This class provides a reinforcement "
#~ "learning agent including training method. "
#~ "This class runs on a single "
#~ "thread."
#~ msgstr ""

#~ msgid "Environment. This must be a child class of ``BaseEnv``."
#~ msgstr ""

#~ msgid "Actor Critic Model."
#~ msgstr ""

#~ msgid "Number of actor/environment model."
#~ msgstr ""

#~ msgid "Advantage steps."
#~ msgstr ""

#~ msgid "node selector."
#~ msgstr ""

#~ msgid "test node selector."
#~ msgstr ""

#~ msgid "Loss function for train q-network. Default is ``MeanSquaredError()``."
#~ msgstr ""

#~ msgid ""
#~ "Optimizer for train q-network. Default "
#~ "is ``Rmsprop(lr=0.00025, g=0.95)``."
#~ msgstr ""

#~ msgid "Coefficient of actor's output entropy."
#~ msgstr ""

#~ msgid "Coefficient of value loss."
#~ msgstr ""

#~ msgid "Discount rate."
#~ msgstr ""

#~ msgid "The size of replay buffer."
#~ msgstr ""

#~ msgid "Example"
#~ msgstr ""

#~ msgid "References"
#~ msgstr ""

#~ msgid "A. V. Clemente, H. N. Castejon, and A. Chandra."
#~ msgstr ""

#~ msgid "Efficient Parallel Methods for Deep Reinforcement Learning"
#~ msgstr ""

#~ msgid "https://arxiv.org/abs/1509.02971"
#~ msgstr ""

#~ msgid ""
#~ "This method executes training of actor"
#~ " critic. Test will be runned after"
#~ " each epoch is done."
#~ msgstr ""

#~ msgid "Number of epoch for training."
#~ msgstr ""

#~ msgid "Number of step of one epoch."
#~ msgstr ""

#~ msgid "Number steps during test."
#~ msgstr ""

#~ msgid "Test the trained actor agent."
#~ msgstr ""

#~ msgid ""
#~ "Number of steps (not episodes) for "
#~ "test. If None is given, this "
#~ "method tests execute only 1 episode."
#~ msgstr ""

#~ msgid "Sum of rewards."
#~ msgstr ""

